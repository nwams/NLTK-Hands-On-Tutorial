{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Text Language by Counting Stop Words¶\n",
    "This is based on Alejandro Nolla's <a href=\"http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/\" target=\"_blank\">Detecting Text Language with Python and NLTK</a>\n",
    "\n",
    "As I mentioned in my <a href=\"https://medium.com/nwamaka-imasogie/stand-up-comedy-and-nlp-c7d64002520c\" target=\"_blank\">Stand-Up Comedy and NLP Python publication</a>, **stop words** are common words that add no additional meaning to text such as 'a', 'the', etc. We often filter them out before doing any kind of processing because they are mostly grammatical (not semantic) in nature.\n",
    "\n",
    "We can use stop words to detect language. Search engines, for example, can show us a text in one particular language we choose, like English or Spanish. To do that the indexed text was analyzed previously to \"guess\" the language and store it together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Tokenizing\n",
    "\n",
    "When tokenizing we have to decide whether we want to break it down to \"words\" or \"tokens\". Some things to think about are whether we should keep contractions or not (for example, _they're_ versus _they are_)In this case I'm going to **split all punctuations into seperate tokens**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Yo man, it's time for you to shut yo' mouth! I ain't even messin' dawg.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yo',\n",
       " 'man',\n",
       " ',',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'time',\n",
       " 'for',\n",
       " 'you',\n",
       " 'to',\n",
       " 'shut',\n",
       " 'yo',\n",
       " \"'\",\n",
       " 'mouth',\n",
       " '!',\n",
       " 'I',\n",
       " 'ain',\n",
       " \"'\",\n",
       " 't',\n",
       " 'even',\n",
       " 'messin',\n",
       " \"'\",\n",
       " 'dawg',\n",
       " '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize # regex-based tokenizer which splits text on whitespace and punctuation (except for underscore)\n",
    "\n",
    "tokens = wordpunct_tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Exploring NLTK's stop words corpus\n",
    "Now I have clean words to match against a  list of stop words. NLTK comes in handy because it comes with a corpus of stop words from various languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopwords Corpus  This corpus contains lists of stop words for several languages.  These are high-frequency grammatical words which are usually ignored in text retrieval applications.  They were obtained from: http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/  The stop words for the Romanian language were obtained from: http://arlc.ro/resources/  The English list has been augmented https://github.com/nltk/nltk_data/issues/22  The German list has been corrected https://github.com/nltk/nltk_data/pull/49  A Kazakh list has been added https://github.com/nltk/nltk_data/pull/52  '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.readme().replace('\\n', ' ') # The contents of the README file of the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids() # Most corpora consist of a set of files. fileids() is a list of identifiers for these files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Corpus readers have a variety of ways to read data from a corpus like .words(), .raw(), and .sents()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'og i jeg det at en den til er som på de med han af for ikke der var mig sig men et har om vi min havde ham hun nu over da fra du ud sin dem os op man hans hvor eller hvad skal selv her alle vil blev kunne ind når være dog noget ville jo deres efter ned skulle denne end dette mit også under have dig anden hende mine alt meget sit sine vor mod disse hvis din nogle hos blive mange ad bliver hendes været thi jer sådan '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.raw('danish').replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "466"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(['english', 'spanish'])) # There are 466 total English and Spanish stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Classification\n",
    "\n",
    "Now I will compute language probability depending on which stopwords are used. Start by looping through the list of stop words in all of the languages. Then check how many stop words our tokenized text contains in each language.\n",
    "\n",
    "The text is finally classified based on the language in which it has the most stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arabic': 0,\n",
       " 'danish': 3,\n",
       " 'dutch': 0,\n",
       " 'english': 8,\n",
       " 'finnish': 0,\n",
       " 'french': 2,\n",
       " 'german': 1,\n",
       " 'hungarian': 1,\n",
       " 'italian': 1,\n",
       " 'kazakh': 0,\n",
       " 'norwegian': 3,\n",
       " 'portuguese': 1,\n",
       " 'romanian': 2,\n",
       " 'russian': 0,\n",
       " 'spanish': 1,\n",
       " 'swedish': 2,\n",
       " 'turkish': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_ratios = {}\n",
    "\n",
    "words = [word.lower() for word in tokens] # lowercase all tokens\n",
    "\n",
    "# Advantage of sets over lists: optimized for checking whether a specific element is contained in the set\n",
    "words_set = set(words)\n",
    "\n",
    "# Compute number of unique stopwords that appear in the text, compute this per each language\n",
    "for language in stopwords.fileids():\n",
    "    stopwords_set = set(stopwords.words(language))\n",
    "    overlapping_words = words_set.intersection(stopwords_set)\n",
    "    language_ratios[language] = len(overlapping_words) # language score\n",
    "    \n",
    "language_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'english'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The key parameter is a function that computes a key that is used to determine how to rank items\n",
    "highest_scoring_language = max(language_ratios, key=language_ratios.get)\n",
    "highest_scoring_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ain', 'for', 'i', 'it', 's', 't', 'to', 'you'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can even see which English stop words were found\n",
    "words_set.intersection(set(stopwords.words(highest_scoring_language)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
