{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Part-of-Speech Tagger\n",
    "In this notebook I'll train a classifier to determine which suffixes are most informative for POS tagging. \n",
    "\n",
    "### What is POS tagging?\n",
    "Part-of-Speech tagging (or POS for short) is labelling each word with their appropriate Part-of-Speech such Noun, Verb, Adjective, Adverb, Pronoun, etc. These word classes (also known as lexical categories) are useful categories for many language processing tasks. \n",
    "\n",
    "### Applications: Text-to-Speech\n",
    "You might be wondering why POS tagging is needed. Let's discuss one example where POS tagging is applied. The word refuse can either be a verb or a noun. E.g. refUSE is a verb meaning _deny_, while REFuse is a noun meaning _trash_. They are not homophones so they have different pronunciations. Thus we need to know which word is being used in order to pronounce the text correctly. This is why text-to-speech applications perform POS-tagging. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the POS tagger using the [Brown](https://www.nltk.org/book/ch02.html#tab-brown-sources) corpus\n",
    "\n",
    "### Why use the Brown Corpus? \n",
    "Because the Brown Corpus was painstakingly \"tagged\" with part-of-speech markers over many years. By the late 70s the tagging was nearly perfect. [[Source](https://en.wikipedia.org/wiki/Part-of-speech_tagging#The_Brown_Corpus)]\n",
    "\n",
    "* It was the first of the modern, computer readable general corpora.\n",
    "* For a long time, Brown and LOB (British) corpora were the only easily available online, so many studies have been done on these corpora.\n",
    "* Studying the same data allows comparison of findings without having to take into consideration possible variation caused by the use of different data. \n",
    "* It consists of about 1 million words of American English text (printed in 1961), made up of 500 samples from randomly chosen publications. Each sample is 2,000 or more words (ending at the first sentence-end after 2,000 words, so that the corpus contains only complete sentences).\n",
    "\n",
    "### Let's briefly explore the Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "from nltk import FreqDist\n",
    "\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.sents(categories=['adventure',\n",
    " 'belles_lettres',\n",
    " 'editorial',\n",
    " 'fiction',\n",
    " 'government',\n",
    " 'hobbies',\n",
    " 'humor',\n",
    " 'learned',\n",
    " 'lore',\n",
    " 'mystery',\n",
    " 'news',\n",
    " 'religion',\n",
    " 'reviews',\n",
    " 'romance',\n",
    " 'science_fiction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BROWN CORPUS  A Standard Corpus of Present-Day Edited American English, for use with Digital Computers.  by W. N. Francis and H. Kucera (1964) Department of Linguistics, Brown University Providence, Rhode Island, USA  Revised 1971, Revised and Amplified 1979  http://www.hit.uib.no/icame/brown/bcm.html  Distributed with the permission of the copyright holder, redistribution permitted. '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.readme().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'),\n",
       " ('Fulton', 'NP-TL'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('an', 'AT'),\n",
       " ('investigation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " (\"Atlanta's\", 'NP$'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'NN'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('``', '``'),\n",
       " ('no', 'AT'),\n",
       " ('evidence', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('that', 'CS')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.tagged_words()[:20] # List of tuples which conatain the word and its POS tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's find out what the most common suffixes are\n",
    "We can train a classifier to work out which suffixes are most informative for POS tagging.\n",
    "\n",
    "Before starting training a classifier, we must agree first on what features to use. Let's use the **2-letter suffix** and the **3-letter suffix**. The 2-letter suffix is a great indicator of past-tense verbs, ending in “-ed”. And the 3-letter suffix helps recognize the present participle ending in “-ing”. \n",
    "\n",
    "I'd like to note that we can do better by also looking at the word itself, the word before and the word after. However, for the scope of this project we'll move forward with just the suffixes. \n",
    "\n",
    "\n",
    "### Create a Frequency Distribution for suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'e': 202808, ',': 175002, '.': 152999, 's': 128590, 'd': 105493, 't': 94237, 'n': 87776, 'he': 86119, 'of': 72314, 'a': 70852, ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "suffix_fdist = FreqDist()\n",
    "\n",
    "# Need a refresher on python array slice notation? \n",
    "# Visit https://stackoverflow.com/questions/509211/understanding-slice-notation\n",
    "for word in brown.words():\n",
    "    suffix_fdist[word[-1:]] += 1 # Keep a count of suffixes containing only one letter\n",
    "    suffix_fdist[word[-2:]] += 1 # Keep a count of suffixes containing two letters\n",
    "    suffix_fdist[word[-3:]] += 1 # Keep a count of suffixes containing three letters\n",
    "    \n",
    "suffix_fdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's preview some of the most common suffixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['e',\n",
       " ',',\n",
       " '.',\n",
       " 's',\n",
       " 'd',\n",
       " 't',\n",
       " 'n',\n",
       " 'he',\n",
       " 'of',\n",
       " 'a',\n",
       " 'the',\n",
       " 'y',\n",
       " 'r',\n",
       " 'to',\n",
       " 'in',\n",
       " 'f',\n",
       " 'o',\n",
       " 'ed',\n",
       " 'nd',\n",
       " 'is',\n",
       " 'on',\n",
       " 'l',\n",
       " 'g',\n",
       " 'and',\n",
       " 'ng',\n",
       " 'er',\n",
       " 'ing',\n",
       " 'as',\n",
       " 'h',\n",
       " 'at',\n",
       " 'es',\n",
       " 'or',\n",
       " 're',\n",
       " '``',\n",
       " \"''\",\n",
       " 'an',\n",
       " 'm',\n",
       " ';',\n",
       " 'ly',\n",
       " 'I',\n",
       " 'it',\n",
       " 'ion',\n",
       " 'en',\n",
       " 'al',\n",
       " '?',\n",
       " 'nt',\n",
       " 'be',\n",
       " 'hat',\n",
       " 'st',\n",
       " 'th',\n",
       " 'his',\n",
       " 'll',\n",
       " 'le',\n",
       " 'ce',\n",
       " 'ts',\n",
       " 've',\n",
       " 'me',\n",
       " 'by',\n",
       " \"'\",\n",
       " 'se',\n",
       " 'ut',\n",
       " 'was',\n",
       " 'ent',\n",
       " 'ch',\n",
       " 'k',\n",
       " 'w',\n",
       " 'ld',\n",
       " 'for',\n",
       " '`',\n",
       " 'rs',\n",
       " 'ted',\n",
       " 'ere',\n",
       " 'ne',\n",
       " 'her',\n",
       " 'ns',\n",
       " 'ith',\n",
       " 'ad',\n",
       " 'ry',\n",
       " ')',\n",
       " '(',\n",
       " 'The',\n",
       " 'te',\n",
       " '--',\n",
       " 'ay',\n",
       " 'ty',\n",
       " 'ot',\n",
       " 'p',\n",
       " 'nce',\n",
       " 'He',\n",
       " \"'s\",\n",
       " 'ter',\n",
       " 'om',\n",
       " 'ss',\n",
       " ':',\n",
       " 'are',\n",
       " 'ers',\n",
       " 'uld',\n",
       " 'had',\n",
       " 'ey',\n",
       " 'ow']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_suffixes = [suffix for (suffix, count) in suffix_fdist.most_common(100)]\n",
    "common_suffixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "### Define a feature extractor function that checks a given word for these suffixes\n",
    "\n",
    "Feature extraction functions highlight some of the properties in our data however they also make it impossible to see other properties. The classifier will rely exclusively on these highlighted properties when determining how to label inputs. In this case, the classifier will make its decisions based only on information about which of the common suffixes (if any) a given word has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(word):\n",
    "    '''Extract features of given word'''\n",
    "    features = {}\n",
    "    for suffix in common_suffixes:\n",
    "        features['endswith({})'.format(suffix)] = word.lower().endswith(suffix) # Returns a True/False if string ends with the specified suffix.\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test it out**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'endswith(e)': False,\n",
       " 'endswith(,)': False,\n",
       " 'endswith(.)': False,\n",
       " 'endswith(s)': False,\n",
       " 'endswith(d)': False,\n",
       " 'endswith(t)': True,\n",
       " 'endswith(n)': False,\n",
       " 'endswith(he)': False,\n",
       " 'endswith(of)': False,\n",
       " 'endswith(a)': False,\n",
       " 'endswith(the)': False,\n",
       " 'endswith(y)': False,\n",
       " 'endswith(r)': False,\n",
       " 'endswith(to)': False,\n",
       " 'endswith(in)': False,\n",
       " 'endswith(f)': False,\n",
       " 'endswith(o)': False,\n",
       " 'endswith(ed)': False,\n",
       " 'endswith(nd)': False,\n",
       " 'endswith(is)': False,\n",
       " 'endswith(on)': False,\n",
       " 'endswith(l)': False,\n",
       " 'endswith(g)': False,\n",
       " 'endswith(and)': False,\n",
       " 'endswith(ng)': False,\n",
       " 'endswith(er)': False,\n",
       " 'endswith(ing)': False,\n",
       " 'endswith(as)': False,\n",
       " 'endswith(h)': False,\n",
       " 'endswith(at)': False,\n",
       " 'endswith(es)': False,\n",
       " 'endswith(or)': False,\n",
       " 'endswith(re)': False,\n",
       " 'endswith(``)': False,\n",
       " \"endswith('')\": False,\n",
       " 'endswith(an)': False,\n",
       " 'endswith(m)': False,\n",
       " 'endswith(;)': False,\n",
       " 'endswith(ly)': False,\n",
       " 'endswith(I)': False,\n",
       " 'endswith(it)': False,\n",
       " 'endswith(ion)': False,\n",
       " 'endswith(en)': False,\n",
       " 'endswith(al)': False,\n",
       " 'endswith(?)': False,\n",
       " 'endswith(nt)': False,\n",
       " 'endswith(be)': False,\n",
       " 'endswith(hat)': False,\n",
       " 'endswith(st)': True,\n",
       " 'endswith(th)': False,\n",
       " 'endswith(his)': False,\n",
       " 'endswith(ll)': False,\n",
       " 'endswith(le)': False,\n",
       " 'endswith(ce)': False,\n",
       " 'endswith(ts)': False,\n",
       " 'endswith(ve)': False,\n",
       " 'endswith(me)': False,\n",
       " 'endswith(by)': False,\n",
       " \"endswith(')\": False,\n",
       " 'endswith(se)': False,\n",
       " 'endswith(ut)': False,\n",
       " 'endswith(was)': False,\n",
       " 'endswith(ent)': False,\n",
       " 'endswith(ch)': False,\n",
       " 'endswith(k)': False,\n",
       " 'endswith(w)': False,\n",
       " 'endswith(ld)': False,\n",
       " 'endswith(for)': False,\n",
       " 'endswith(`)': False,\n",
       " 'endswith(rs)': False,\n",
       " 'endswith(ted)': False,\n",
       " 'endswith(ere)': False,\n",
       " 'endswith(ne)': False,\n",
       " 'endswith(her)': False,\n",
       " 'endswith(ns)': False,\n",
       " 'endswith(ith)': False,\n",
       " 'endswith(ad)': False,\n",
       " 'endswith(ry)': False,\n",
       " 'endswith())': False,\n",
       " 'endswith(()': False,\n",
       " 'endswith(The)': False,\n",
       " 'endswith(te)': False,\n",
       " 'endswith(--)': False,\n",
       " 'endswith(ay)': False,\n",
       " 'endswith(ty)': False,\n",
       " 'endswith(ot)': False,\n",
       " 'endswith(p)': False,\n",
       " 'endswith(nce)': False,\n",
       " 'endswith(He)': False,\n",
       " \"endswith('s)\": False,\n",
       " 'endswith(ter)': False,\n",
       " 'endswith(om)': False,\n",
       " 'endswith(ss)': False,\n",
       " 'endswith(:)': False,\n",
       " 'endswith(are)': False,\n",
       " 'endswith(ers)': False,\n",
       " 'endswith(uld)': False,\n",
       " 'endswith(had)': False,\n",
       " 'endswith(ey)': False,\n",
       " 'endswith(ow)': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_features('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Decision Tree Classifier\n",
    "Here's a [cool video](https://www.youtube.com/watch?v=LDRbO9a6XPU) that talks about Decision Tree Classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'AT'), ('Fulton', 'NP-TL'), ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_words = brown.tagged_words(categories='news')\n",
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'endswith(e)': True,\n",
       "  'endswith(,)': False,\n",
       "  'endswith(.)': False,\n",
       "  'endswith(s)': False,\n",
       "  'endswith(d)': False,\n",
       "  'endswith(t)': False,\n",
       "  'endswith(n)': False,\n",
       "  'endswith(he)': True,\n",
       "  'endswith(of)': False,\n",
       "  'endswith(a)': False,\n",
       "  'endswith(the)': True,\n",
       "  'endswith(y)': False,\n",
       "  'endswith(r)': False,\n",
       "  'endswith(to)': False,\n",
       "  'endswith(in)': False,\n",
       "  'endswith(f)': False,\n",
       "  'endswith(o)': False,\n",
       "  'endswith(ed)': False,\n",
       "  'endswith(nd)': False,\n",
       "  'endswith(is)': False,\n",
       "  'endswith(on)': False,\n",
       "  'endswith(l)': False,\n",
       "  'endswith(g)': False,\n",
       "  'endswith(and)': False,\n",
       "  'endswith(ng)': False,\n",
       "  'endswith(er)': False,\n",
       "  'endswith(ing)': False,\n",
       "  'endswith(as)': False,\n",
       "  'endswith(h)': False,\n",
       "  'endswith(at)': False,\n",
       "  'endswith(es)': False,\n",
       "  'endswith(or)': False,\n",
       "  'endswith(re)': False,\n",
       "  'endswith(``)': False,\n",
       "  \"endswith('')\": False,\n",
       "  'endswith(an)': False,\n",
       "  'endswith(m)': False,\n",
       "  'endswith(;)': False,\n",
       "  'endswith(ly)': False,\n",
       "  'endswith(I)': False,\n",
       "  'endswith(it)': False,\n",
       "  'endswith(ion)': False,\n",
       "  'endswith(en)': False,\n",
       "  'endswith(al)': False,\n",
       "  'endswith(?)': False,\n",
       "  'endswith(nt)': False,\n",
       "  'endswith(be)': False,\n",
       "  'endswith(hat)': False,\n",
       "  'endswith(st)': False,\n",
       "  'endswith(th)': False,\n",
       "  'endswith(his)': False,\n",
       "  'endswith(ll)': False,\n",
       "  'endswith(le)': False,\n",
       "  'endswith(ce)': False,\n",
       "  'endswith(ts)': False,\n",
       "  'endswith(ve)': False,\n",
       "  'endswith(me)': False,\n",
       "  'endswith(by)': False,\n",
       "  \"endswith(')\": False,\n",
       "  'endswith(se)': False,\n",
       "  'endswith(ut)': False,\n",
       "  'endswith(was)': False,\n",
       "  'endswith(ent)': False,\n",
       "  'endswith(ch)': False,\n",
       "  'endswith(k)': False,\n",
       "  'endswith(w)': False,\n",
       "  'endswith(ld)': False,\n",
       "  'endswith(for)': False,\n",
       "  'endswith(`)': False,\n",
       "  'endswith(rs)': False,\n",
       "  'endswith(ted)': False,\n",
       "  'endswith(ere)': False,\n",
       "  'endswith(ne)': False,\n",
       "  'endswith(her)': False,\n",
       "  'endswith(ns)': False,\n",
       "  'endswith(ith)': False,\n",
       "  'endswith(ad)': False,\n",
       "  'endswith(ry)': False,\n",
       "  'endswith())': False,\n",
       "  'endswith(()': False,\n",
       "  'endswith(The)': False,\n",
       "  'endswith(te)': False,\n",
       "  'endswith(--)': False,\n",
       "  'endswith(ay)': False,\n",
       "  'endswith(ty)': False,\n",
       "  'endswith(ot)': False,\n",
       "  'endswith(p)': False,\n",
       "  'endswith(nce)': False,\n",
       "  'endswith(He)': False,\n",
       "  \"endswith('s)\": False,\n",
       "  'endswith(ter)': False,\n",
       "  'endswith(om)': False,\n",
       "  'endswith(ss)': False,\n",
       "  'endswith(:)': False,\n",
       "  'endswith(are)': False,\n",
       "  'endswith(ers)': False,\n",
       "  'endswith(uld)': False,\n",
       "  'endswith(had)': False,\n",
       "  'endswith(ey)': False,\n",
       "  'endswith(ow)': False},\n",
       " 'AT')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresets = [(get_features(n), g) for (n,g) in tagged_words]\n",
    "featuresets[0] # Preview the first element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split the train/test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(len(featuresets) * 0.2) \n",
    "train_set, test_set = featuresets[cutoff:], featuresets[:cutoff] # train on 80%, test on 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the Classifier (took about 15 minutes to complete)**\n",
    "\n",
    "NLTK is a teaching toolkit which is not really optimized for speed. Therefore, this may take a while. For speed, use [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) for the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.624763799104923"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import DecisionTreeClassifier # https://www.nltk.org/book/ch06.html#sec-decision-trees\n",
    "from nltk.classify import accuracy\n",
    "\n",
    "classifier = DecisionTreeClassifier.train(train_set)\n",
    "accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy = 62%. That's not great. To improve the classifier, if we worked with tagged sentences instead of tagged words we can add more contextual features as I mentioned before, like the word itself, the word before and the word after. As well as the previous tag! For the scope of this project, we'll stop right here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we can use our classifier:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NNS'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.classify(get_features('cats'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It correctly predicted that the Part-of-Speech for \"cats\" is 'NNS' which is a plural noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudocode\n",
    "NLTK can print out the decision tree's steps as pseudocode so that it's fairly easy to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if endswith(the) == False: \n",
      "  if endswith(,) == False: \n",
      "    if endswith(s) == False: \n",
      "      if endswith(.) == False: return '.'\n",
      "      if endswith(.) == True: return '.'\n",
      "    if endswith(s) == True: \n",
      "      if endswith(was) == False: return 'PP$'\n",
      "      if endswith(was) == True: return 'BEDZ'\n",
      "  if endswith(,) == True: return ','\n",
      "if endswith(the) == True: return 'AT'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classifier.pseudocode(depth=4)) # depth=4 argument just displays the top portion of the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the classifier begins by checking whether a word end with \"the\". If so, it is tagged \"AT\". If it does not end eith \"the\" the classifier checks if the word does ends with a comma. If it does it will receive the special \",\" tag. If it does not end with a comma the classifier continues on to check if the word doesn't end in \"s\". If not, then either way it's most likely a punctuation mark \".\".  If it does end with \"s\" it will check if the word is \"was\". If the word is not \"was\" then it will receive the Posessive Pronoun tag \"PRP$\". If the end word is \"was\" it will receive the special tag \"BEDZ\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
