{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deriving N-Grams from Text\n",
    "\n",
    "<a href=\"http://cloudmark.github.io/Language-Detection/\" target=\"_blank\">What exactly is an N-gram?</a> An N-Gram is an N-character slice of a longer string. We'll pad the beginning and end of our N-grams with blank underscores. For example: \n",
    "* The **bigram** for the word **text** would be: \"\\_t\", \"te\", \"ex\", \"xt\", \"t\\_\"\n",
    "* The **trigram** for the word **text** would be: \"\\_te\", \"tex\", \"ext\", \"xt\\_\", \"t\\_\\_\"\n",
    "\n",
    "N-Gram-based text categorization is useful for detecting the language that a piece of text belongs to. It is also useful for identifying the topic of the text. This tutorial is based on this <a href=\"http://blog.alejandronolla.com/2013/05/20/n-gram-based-text-categorization-categorizing-text-with-python/\" target=\"_blank\">N-Gram-based Text Categorizer proof of concept built in Python</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenization\n",
    "The first step is to tokenize by splitting a string into substrings. We can use <a href=\"http://www.nltk.org/api/nltk.tokenize.html?highlight=regexp#module-nltk.tokenize.regexp\" target=\"_blank\">NLTK's regexp</a> module for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves.\"\n",
    "s = s.lower()\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['le',\n",
       " 'temps',\n",
       " 'est',\n",
       " 'un',\n",
       " 'grand',\n",
       " 'maître',\n",
       " 'dit',\n",
       " 'on',\n",
       " 'le',\n",
       " 'malheur',\n",
       " 'est',\n",
       " \"qu'il\",\n",
       " 'tue',\n",
       " 'ses',\n",
       " 'élèves']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(\"[a-zA-Z'`éèî]+\")\n",
    "s_tokenized = tokenizer.tokenize(s)\n",
    "s_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining n-grams (n=4)\n",
    "**The second step is Generating N-grams for each token (with n=4).**\n",
    "\n",
    "We will also append blanks to the beginning and end of strings in order to help with matching the beginning-of-word and end-of-word situations.\n",
    "\n",
    "NLTK has an <a href=\"http://www.nltk.org/api/nltk.html?highlight=ngrams#nltk.util.ngrams\" target=\"_blank\">ngram module</a>.\n",
    "\n",
    "Side note: Because n is usually chosen as 2 or 3, you might be wondering _At what degree of n do n-grams become counterproductive?_ \n",
    "That'll depend on the perplexity vs. n-gram plot. The [perplexity](https://en.wikipedia.org/wiki/Perplexity) depends on your language model, n-gram size, and data set. As usual, there is a trade-off between the quality of the language model, and how long it takes to run. The best language models nowadays are based on neural networks, so the choice of n-gram size is less of an issue. [<a href=\"https://stats.stackexchange.com/questions/23429/at-what-n-do-n-grams-become-counterproductive\" target=\"_blank\">Source</a>]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('_', '_', '_', 'l'),\n",
       "  ('_', '_', 'l', 'e'),\n",
       "  ('_', 'l', 'e', '_'),\n",
       "  ('l', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 't'),\n",
       "  ('_', '_', 't', 'e'),\n",
       "  ('_', 't', 'e', 'm'),\n",
       "  ('t', 'e', 'm', 'p'),\n",
       "  ('e', 'm', 'p', 's'),\n",
       "  ('m', 'p', 's', '_'),\n",
       "  ('p', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')],\n",
       " [('_', '_', '_', 'e'),\n",
       "  ('_', '_', 'e', 's'),\n",
       "  ('_', 'e', 's', 't'),\n",
       "  ('e', 's', 't', '_'),\n",
       "  ('s', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'u'),\n",
       "  ('_', '_', 'u', 'n'),\n",
       "  ('_', 'u', 'n', '_'),\n",
       "  ('u', 'n', '_', '_'),\n",
       "  ('n', '_', '_', '_')],\n",
       " [('_', '_', '_', 'g'),\n",
       "  ('_', '_', 'g', 'r'),\n",
       "  ('_', 'g', 'r', 'a'),\n",
       "  ('g', 'r', 'a', 'n'),\n",
       "  ('r', 'a', 'n', 'd'),\n",
       "  ('a', 'n', 'd', '_'),\n",
       "  ('n', 'd', '_', '_'),\n",
       "  ('d', '_', '_', '_')],\n",
       " [('_', '_', '_', 'm'),\n",
       "  ('_', '_', 'm', 'a'),\n",
       "  ('_', 'm', 'a', 'î'),\n",
       "  ('m', 'a', 'î', 't'),\n",
       "  ('a', 'î', 't', 'r'),\n",
       "  ('î', 't', 'r', 'e'),\n",
       "  ('t', 'r', 'e', '_'),\n",
       "  ('r', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 'd'),\n",
       "  ('_', '_', 'd', 'i'),\n",
       "  ('_', 'd', 'i', 't'),\n",
       "  ('d', 'i', 't', '_'),\n",
       "  ('i', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'o'),\n",
       "  ('_', '_', 'o', 'n'),\n",
       "  ('_', 'o', 'n', '_'),\n",
       "  ('o', 'n', '_', '_'),\n",
       "  ('n', '_', '_', '_')],\n",
       " [('_', '_', '_', 'l'),\n",
       "  ('_', '_', 'l', 'e'),\n",
       "  ('_', 'l', 'e', '_'),\n",
       "  ('l', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 'm'),\n",
       "  ('_', '_', 'm', 'a'),\n",
       "  ('_', 'm', 'a', 'l'),\n",
       "  ('m', 'a', 'l', 'h'),\n",
       "  ('a', 'l', 'h', 'e'),\n",
       "  ('l', 'h', 'e', 'u'),\n",
       "  ('h', 'e', 'u', 'r'),\n",
       "  ('e', 'u', 'r', '_'),\n",
       "  ('u', 'r', '_', '_'),\n",
       "  ('r', '_', '_', '_')],\n",
       " [('_', '_', '_', 'e'),\n",
       "  ('_', '_', 'e', 's'),\n",
       "  ('_', 'e', 's', 't'),\n",
       "  ('e', 's', 't', '_'),\n",
       "  ('s', 't', '_', '_'),\n",
       "  ('t', '_', '_', '_')],\n",
       " [('_', '_', '_', 'q'),\n",
       "  ('_', '_', 'q', 'u'),\n",
       "  ('_', 'q', 'u', \"'\"),\n",
       "  ('q', 'u', \"'\", 'i'),\n",
       "  ('u', \"'\", 'i', 'l'),\n",
       "  (\"'\", 'i', 'l', '_'),\n",
       "  ('i', 'l', '_', '_'),\n",
       "  ('l', '_', '_', '_')],\n",
       " [('_', '_', '_', 't'),\n",
       "  ('_', '_', 't', 'u'),\n",
       "  ('_', 't', 'u', 'e'),\n",
       "  ('t', 'u', 'e', '_'),\n",
       "  ('u', 'e', '_', '_'),\n",
       "  ('e', '_', '_', '_')],\n",
       " [('_', '_', '_', 's'),\n",
       "  ('_', '_', 's', 'e'),\n",
       "  ('_', 's', 'e', 's'),\n",
       "  ('s', 'e', 's', '_'),\n",
       "  ('e', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')],\n",
       " [('_', '_', '_', 'é'),\n",
       "  ('_', '_', 'é', 'l'),\n",
       "  ('_', 'é', 'l', 'è'),\n",
       "  ('é', 'l', 'è', 'v'),\n",
       "  ('l', 'è', 'v', 'e'),\n",
       "  ('è', 'v', 'e', 's'),\n",
       "  ('v', 'e', 's', '_'),\n",
       "  ('e', 's', '_', '_'),\n",
       "  ('s', '_', '_', '_')]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "n=4\n",
    "generated_n_grams = []\n",
    "for word in s_tokenized:\n",
    "    generated_n_grams.append(list(ngrams(word, n, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')))\n",
    "    \n",
    "generated_n_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Currently this is a list of lists of 4 grams. So flatten this to it's just list of 4-grams**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', '_', '_', 'l'),\n",
       " ('_', '_', 'l', 'e'),\n",
       " ('_', 'l', 'e', '_'),\n",
       " ('l', 'e', '_', '_'),\n",
       " ('e', '_', '_', '_'),\n",
       " ('_', '_', '_', 't'),\n",
       " ('_', '_', 't', 'e'),\n",
       " ('_', 't', 'e', 'm'),\n",
       " ('t', 'e', 'm', 'p'),\n",
       " ('e', 'm', 'p', 's'),\n",
       " ('m', 'p', 's', '_'),\n",
       " ('p', 's', '_', '_'),\n",
       " ('s', '_', '_', '_'),\n",
       " ('_', '_', '_', 'e'),\n",
       " ('_', '_', 'e', 's'),\n",
       " ('_', 'e', 's', 't'),\n",
       " ('e', 's', 't', '_'),\n",
       " ('s', 't', '_', '_'),\n",
       " ('t', '_', '_', '_'),\n",
       " ('_', '_', '_', 'u'),\n",
       " ('_', '_', 'u', 'n'),\n",
       " ('_', 'u', 'n', '_'),\n",
       " ('u', 'n', '_', '_'),\n",
       " ('n', '_', '_', '_'),\n",
       " ('_', '_', '_', 'g'),\n",
       " ('_', '_', 'g', 'r'),\n",
       " ('_', 'g', 'r', 'a'),\n",
       " ('g', 'r', 'a', 'n'),\n",
       " ('r', 'a', 'n', 'd'),\n",
       " ('a', 'n', 'd', '_'),\n",
       " ('n', 'd', '_', '_'),\n",
       " ('d', '_', '_', '_'),\n",
       " ('_', '_', '_', 'm'),\n",
       " ('_', '_', 'm', 'a'),\n",
       " ('_', 'm', 'a', 'î'),\n",
       " ('m', 'a', 'î', 't'),\n",
       " ('a', 'î', 't', 'r'),\n",
       " ('î', 't', 'r', 'e'),\n",
       " ('t', 'r', 'e', '_'),\n",
       " ('r', 'e', '_', '_'),\n",
       " ('e', '_', '_', '_'),\n",
       " ('_', '_', '_', 'd'),\n",
       " ('_', '_', 'd', 'i'),\n",
       " ('_', 'd', 'i', 't'),\n",
       " ('d', 'i', 't', '_'),\n",
       " ('i', 't', '_', '_'),\n",
       " ('t', '_', '_', '_'),\n",
       " ('_', '_', '_', 'o'),\n",
       " ('_', '_', 'o', 'n'),\n",
       " ('_', 'o', 'n', '_'),\n",
       " ('o', 'n', '_', '_'),\n",
       " ('n', '_', '_', '_'),\n",
       " ('_', '_', '_', 'l'),\n",
       " ('_', '_', 'l', 'e'),\n",
       " ('_', 'l', 'e', '_'),\n",
       " ('l', 'e', '_', '_'),\n",
       " ('e', '_', '_', '_'),\n",
       " ('_', '_', '_', 'm'),\n",
       " ('_', '_', 'm', 'a'),\n",
       " ('_', 'm', 'a', 'l'),\n",
       " ('m', 'a', 'l', 'h'),\n",
       " ('a', 'l', 'h', 'e'),\n",
       " ('l', 'h', 'e', 'u'),\n",
       " ('h', 'e', 'u', 'r'),\n",
       " ('e', 'u', 'r', '_'),\n",
       " ('u', 'r', '_', '_'),\n",
       " ('r', '_', '_', '_'),\n",
       " ('_', '_', '_', 'e'),\n",
       " ('_', '_', 'e', 's'),\n",
       " ('_', 'e', 's', 't'),\n",
       " ('e', 's', 't', '_'),\n",
       " ('s', 't', '_', '_'),\n",
       " ('t', '_', '_', '_'),\n",
       " ('_', '_', '_', 'q'),\n",
       " ('_', '_', 'q', 'u'),\n",
       " ('_', 'q', 'u', \"'\"),\n",
       " ('q', 'u', \"'\", 'i'),\n",
       " ('u', \"'\", 'i', 'l'),\n",
       " (\"'\", 'i', 'l', '_'),\n",
       " ('i', 'l', '_', '_'),\n",
       " ('l', '_', '_', '_'),\n",
       " ('_', '_', '_', 't'),\n",
       " ('_', '_', 't', 'u'),\n",
       " ('_', 't', 'u', 'e'),\n",
       " ('t', 'u', 'e', '_'),\n",
       " ('u', 'e', '_', '_'),\n",
       " ('e', '_', '_', '_'),\n",
       " ('_', '_', '_', 's'),\n",
       " ('_', '_', 's', 'e'),\n",
       " ('_', 's', 'e', 's'),\n",
       " ('s', 'e', 's', '_'),\n",
       " ('e', 's', '_', '_'),\n",
       " ('s', '_', '_', '_'),\n",
       " ('_', '_', '_', 'é'),\n",
       " ('_', '_', 'é', 'l'),\n",
       " ('_', 'é', 'l', 'è'),\n",
       " ('é', 'l', 'è', 'v'),\n",
       " ('l', 'è', 'v', 'e'),\n",
       " ('è', 'v', 'e', 's'),\n",
       " ('v', 'e', 's', '_'),\n",
       " ('e', 's', '_', '_'),\n",
       " ('s', '_', '_', '_')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_list_of_ngrams = []\n",
    "for sublist in generated_n_grams:\n",
    "    for item in sublist:\n",
    "        flat_list_of_ngrams.append(item)\n",
    "\n",
    "flat_list_of_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now we need to join the elements within a tuple to get just one list of n-grams.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertTuple(tup):\n",
    "    '''Takes a tuple and converts it into a string'''\n",
    "    str =  ''.join(tup) \n",
    "    return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['___l',\n",
       " '__le',\n",
       " '_le_',\n",
       " 'le__',\n",
       " 'e___',\n",
       " '___t',\n",
       " '__te',\n",
       " '_tem',\n",
       " 'temp',\n",
       " 'emps',\n",
       " 'mps_',\n",
       " 'ps__',\n",
       " 's___',\n",
       " '___e',\n",
       " '__es',\n",
       " '_est',\n",
       " 'est_',\n",
       " 'st__',\n",
       " 't___',\n",
       " '___u',\n",
       " '__un',\n",
       " '_un_',\n",
       " 'un__',\n",
       " 'n___',\n",
       " '___g',\n",
       " '__gr',\n",
       " '_gra',\n",
       " 'gran',\n",
       " 'rand',\n",
       " 'and_',\n",
       " 'nd__',\n",
       " 'd___',\n",
       " '___m',\n",
       " '__ma',\n",
       " '_maî',\n",
       " 'maît',\n",
       " 'aîtr',\n",
       " 'ître',\n",
       " 'tre_',\n",
       " 're__',\n",
       " 'e___',\n",
       " '___d',\n",
       " '__di',\n",
       " '_dit',\n",
       " 'dit_',\n",
       " 'it__',\n",
       " 't___',\n",
       " '___o',\n",
       " '__on',\n",
       " '_on_',\n",
       " 'on__',\n",
       " 'n___',\n",
       " '___l',\n",
       " '__le',\n",
       " '_le_',\n",
       " 'le__',\n",
       " 'e___',\n",
       " '___m',\n",
       " '__ma',\n",
       " '_mal',\n",
       " 'malh',\n",
       " 'alhe',\n",
       " 'lheu',\n",
       " 'heur',\n",
       " 'eur_',\n",
       " 'ur__',\n",
       " 'r___',\n",
       " '___e',\n",
       " '__es',\n",
       " '_est',\n",
       " 'est_',\n",
       " 'st__',\n",
       " 't___',\n",
       " '___q',\n",
       " '__qu',\n",
       " \"_qu'\",\n",
       " \"qu'i\",\n",
       " \"u'il\",\n",
       " \"'il_\",\n",
       " 'il__',\n",
       " 'l___',\n",
       " '___t',\n",
       " '__tu',\n",
       " '_tue',\n",
       " 'tue_',\n",
       " 'ue__',\n",
       " 'e___',\n",
       " '___s',\n",
       " '__se',\n",
       " '_ses',\n",
       " 'ses_',\n",
       " 'es__',\n",
       " 's___',\n",
       " '___é',\n",
       " '__él',\n",
       " '_élè',\n",
       " 'élèv',\n",
       " 'lève',\n",
       " 'èves',\n",
       " 'ves_',\n",
       " 'es__',\n",
       " 's___']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_quadgrams = [] \n",
    "\n",
    "for line in flat_list_of_ngrams:\n",
    "    list_of_quadgrams.append(convertTuple(line))\n",
    "list_of_quadgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Sorting n-grams in descending order of frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's count each quad-gram occurrance and create a dictionary to hold the counts. \n",
    "I will do a sum when the quad-gram has been seen before or create a new key otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'___l': 2,\n",
       " '__le': 2,\n",
       " '_le_': 2,\n",
       " 'le__': 2,\n",
       " 'e___': 4,\n",
       " '___t': 2,\n",
       " '__te': 1,\n",
       " '_tem': 1,\n",
       " 'temp': 1,\n",
       " 'emps': 1,\n",
       " 'mps_': 1,\n",
       " 'ps__': 1,\n",
       " 's___': 3,\n",
       " '___e': 2,\n",
       " '__es': 2,\n",
       " '_est': 2,\n",
       " 'est_': 2,\n",
       " 'st__': 2,\n",
       " 't___': 3,\n",
       " '___u': 1,\n",
       " '__un': 1,\n",
       " '_un_': 1,\n",
       " 'un__': 1,\n",
       " 'n___': 2,\n",
       " '___g': 1,\n",
       " '__gr': 1,\n",
       " '_gra': 1,\n",
       " 'gran': 1,\n",
       " 'rand': 1,\n",
       " 'and_': 1,\n",
       " 'nd__': 1,\n",
       " 'd___': 1,\n",
       " '___m': 2,\n",
       " '__ma': 2,\n",
       " '_maî': 1,\n",
       " 'maît': 1,\n",
       " 'aîtr': 1,\n",
       " 'ître': 1,\n",
       " 'tre_': 1,\n",
       " 're__': 1,\n",
       " '___d': 1,\n",
       " '__di': 1,\n",
       " '_dit': 1,\n",
       " 'dit_': 1,\n",
       " 'it__': 1,\n",
       " '___o': 1,\n",
       " '__on': 1,\n",
       " '_on_': 1,\n",
       " 'on__': 1,\n",
       " '_mal': 1,\n",
       " 'malh': 1,\n",
       " 'alhe': 1,\n",
       " 'lheu': 1,\n",
       " 'heur': 1,\n",
       " 'eur_': 1,\n",
       " 'ur__': 1,\n",
       " 'r___': 1,\n",
       " '___q': 1,\n",
       " '__qu': 1,\n",
       " \"_qu'\": 1,\n",
       " \"qu'i\": 1,\n",
       " \"u'il\": 1,\n",
       " \"'il_\": 1,\n",
       " 'il__': 1,\n",
       " 'l___': 1,\n",
       " '__tu': 1,\n",
       " '_tue': 1,\n",
       " 'tue_': 1,\n",
       " 'ue__': 1,\n",
       " '___s': 1,\n",
       " '__se': 1,\n",
       " '_ses': 1,\n",
       " 'ses_': 1,\n",
       " 'es__': 2,\n",
       " '___é': 1,\n",
       " '__él': 1,\n",
       " '_élè': 1,\n",
       " 'élèv': 1,\n",
       " 'lève': 1,\n",
       " 'èves': 1,\n",
       " 'ves_': 1}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_n_grams = {}\n",
    "\n",
    "for ngram in list_of_quadgrams:\n",
    "    if ngram not in freq_n_grams: \n",
    "        freq_n_grams.update({ngram: 1})\n",
    "    else: \n",
    "        ngram_occurances = freq_n_grams[ngram]\n",
    "        freq_n_grams.update({ngram: ngram_occurances + 1})\n",
    "    \n",
    "freq_n_grams # Dictionary containting count of quadgrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will sort in descending order, to keep just the top 300 most repeated quad-grams. \n",
    "\n",
    "> * From other language detection frameworks implemented we know that we should expect that the **top 300** or so N-grams are almost always highly correlated to the language. Thus the _language profile_ of a sport document will be very similar to the _language profile_ generated from a policital document in the same language. This gives us confidence that we will still be able to classify documents to the correct language even if they have completely different topics regardless of what document we trained it with. [<a href=\"http://cloudmark.github.io/Language-Detection/\" target=\"_blank\">Source</a>]\n",
    "> * Starting at around rank 300 or so, an N-gram frequency profile begins to become *specific to the topic*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('e___', 4),\n",
       " ('s___', 3),\n",
       " ('t___', 3),\n",
       " ('___l', 2),\n",
       " ('__le', 2),\n",
       " ('_le_', 2),\n",
       " ('le__', 2),\n",
       " ('___t', 2),\n",
       " ('___e', 2),\n",
       " ('__es', 2),\n",
       " ('_est', 2),\n",
       " ('est_', 2),\n",
       " ('st__', 2),\n",
       " ('n___', 2),\n",
       " ('___m', 2),\n",
       " ('__ma', 2),\n",
       " ('es__', 2),\n",
       " ('__te', 1),\n",
       " ('_tem', 1),\n",
       " ('temp', 1),\n",
       " ('emps', 1),\n",
       " ('mps_', 1),\n",
       " ('ps__', 1),\n",
       " ('___u', 1),\n",
       " ('__un', 1),\n",
       " ('_un_', 1),\n",
       " ('un__', 1),\n",
       " ('___g', 1),\n",
       " ('__gr', 1),\n",
       " ('_gra', 1),\n",
       " ('gran', 1),\n",
       " ('rand', 1),\n",
       " ('and_', 1),\n",
       " ('nd__', 1),\n",
       " ('d___', 1),\n",
       " ('_maî', 1),\n",
       " ('maît', 1),\n",
       " ('aîtr', 1),\n",
       " ('ître', 1),\n",
       " ('tre_', 1),\n",
       " ('re__', 1),\n",
       " ('___d', 1),\n",
       " ('__di', 1),\n",
       " ('_dit', 1),\n",
       " ('dit_', 1),\n",
       " ('it__', 1),\n",
       " ('___o', 1),\n",
       " ('__on', 1),\n",
       " ('_on_', 1),\n",
       " ('on__', 1),\n",
       " ('_mal', 1),\n",
       " ('malh', 1),\n",
       " ('alhe', 1),\n",
       " ('lheu', 1),\n",
       " ('heur', 1),\n",
       " ('eur_', 1),\n",
       " ('ur__', 1),\n",
       " ('r___', 1),\n",
       " ('___q', 1),\n",
       " ('__qu', 1),\n",
       " (\"_qu'\", 1),\n",
       " (\"qu'i\", 1),\n",
       " (\"u'il\", 1),\n",
       " (\"'il_\", 1),\n",
       " ('il__', 1),\n",
       " ('l___', 1),\n",
       " ('__tu', 1),\n",
       " ('_tue', 1),\n",
       " ('tue_', 1),\n",
       " ('ue__', 1),\n",
       " ('___s', 1),\n",
       " ('__se', 1),\n",
       " ('_ses', 1),\n",
       " ('ses_', 1),\n",
       " ('___é', 1),\n",
       " ('__él', 1),\n",
       " ('_élè', 1),\n",
       " ('élèv', 1),\n",
       " ('lève', 1),\n",
       " ('èves', 1),\n",
       " ('ves_', 1)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "# Python dicts can't be sorted, so we need to transform it into a sorted list using the operator module\n",
    "# The operator module exports a set of efficient functions corresponding to the intrinsic operators of Python. For example operator.add(a, b) is equivalent to the expression a + b.\n",
    "\n",
    "# Operator.itemgetter(n) constructs a callable that assumes an iterable object (e.g. list, tuple, set) as input, and fetches the n-th element out of it.\n",
    "freq_n_grams_sorted = sorted(freq_n_grams.items(), key=itemgetter(1), reverse=True)[0:300] \n",
    "\n",
    "freq_n_grams_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Getting n-grams for multiple values of n\n",
    "To get n-grams for n = 1, 2, 3 and 4, we can use NLTK's <a href=\"https://tedboy.github.io/nlps/generated/generated/nltk.everygrams.html\" target=\"_blank\">everygrams</a> module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"le temps est un grand maître dit on le malheur est qu'il tue ses élèves\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We need the full raw sentence (still with no punctuation), as opposed to just the tokens.\n",
    "s_clean = ' '.join(s_tokenized)\n",
    "s_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l',\n",
       " 'e',\n",
       " 't',\n",
       " 'e',\n",
       " 'm',\n",
       " 'p',\n",
       " 's',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'n',\n",
       " 'g',\n",
       " 'r',\n",
       " 'a',\n",
       " 'n',\n",
       " 'd',\n",
       " 'm',\n",
       " 'a',\n",
       " 'î',\n",
       " 't',\n",
       " 'r',\n",
       " 'e',\n",
       " 'd',\n",
       " 'i',\n",
       " 't',\n",
       " 'o',\n",
       " 'n',\n",
       " 'l',\n",
       " 'e',\n",
       " 'm',\n",
       " 'a',\n",
       " 'l',\n",
       " 'h',\n",
       " 'e',\n",
       " 'u',\n",
       " 'r',\n",
       " 'e',\n",
       " 's',\n",
       " 't',\n",
       " 'q',\n",
       " 'u',\n",
       " \"'\",\n",
       " 'i',\n",
       " 'l',\n",
       " 't',\n",
       " 'u',\n",
       " 'e',\n",
       " 's',\n",
       " 'e',\n",
       " 's',\n",
       " 'é',\n",
       " 'l',\n",
       " 'è',\n",
       " 'v',\n",
       " 'e',\n",
       " 's',\n",
       " 'le',\n",
       " 'e_',\n",
       " '_t',\n",
       " 'te',\n",
       " 'em',\n",
       " 'mp',\n",
       " 'ps',\n",
       " 's_',\n",
       " '_e',\n",
       " 'es',\n",
       " 'st',\n",
       " 't_',\n",
       " '_u',\n",
       " 'un',\n",
       " 'n_',\n",
       " '_g',\n",
       " 'gr',\n",
       " 'ra',\n",
       " 'an',\n",
       " 'nd',\n",
       " 'd_',\n",
       " '_m',\n",
       " 'ma',\n",
       " 'aî',\n",
       " 'ît',\n",
       " 'tr',\n",
       " 're',\n",
       " 'e_',\n",
       " '_d',\n",
       " 'di',\n",
       " 'it',\n",
       " 't_',\n",
       " '_o',\n",
       " 'on',\n",
       " 'n_',\n",
       " '_l',\n",
       " 'le',\n",
       " 'e_',\n",
       " '_m',\n",
       " 'ma',\n",
       " 'al',\n",
       " 'lh',\n",
       " 'he',\n",
       " 'eu',\n",
       " 'ur',\n",
       " 'r_',\n",
       " '_e',\n",
       " 'es',\n",
       " 'st',\n",
       " 't_',\n",
       " '_q',\n",
       " 'qu',\n",
       " \"u'\",\n",
       " \"'i\",\n",
       " 'il',\n",
       " 'l_',\n",
       " '_t',\n",
       " 'tu',\n",
       " 'ue',\n",
       " 'e_',\n",
       " '_s',\n",
       " 'se',\n",
       " 'es',\n",
       " 's_',\n",
       " '_é',\n",
       " 'él',\n",
       " 'lè',\n",
       " 'èv',\n",
       " 've',\n",
       " 'es',\n",
       " 'le_',\n",
       " '_te',\n",
       " 'tem',\n",
       " 'emp',\n",
       " 'mps',\n",
       " 'ps_',\n",
       " '_es',\n",
       " 'est',\n",
       " 'st_',\n",
       " '_un',\n",
       " 'un_',\n",
       " '_gr',\n",
       " 'gra',\n",
       " 'ran',\n",
       " 'and',\n",
       " 'nd_',\n",
       " '_ma',\n",
       " 'maî',\n",
       " 'aît',\n",
       " 'îtr',\n",
       " 'tre',\n",
       " 're_',\n",
       " '_di',\n",
       " 'dit',\n",
       " 'it_',\n",
       " '_on',\n",
       " 'on_',\n",
       " '_le',\n",
       " 'le_',\n",
       " '_ma',\n",
       " 'mal',\n",
       " 'alh',\n",
       " 'lhe',\n",
       " 'heu',\n",
       " 'eur',\n",
       " 'ur_',\n",
       " '_es',\n",
       " 'est',\n",
       " 'st_',\n",
       " '_qu',\n",
       " \"qu'\",\n",
       " \"u'i\",\n",
       " \"'il\",\n",
       " 'il_',\n",
       " '_tu',\n",
       " 'tue',\n",
       " 'ue_',\n",
       " '_se',\n",
       " 'ses',\n",
       " 'es_',\n",
       " '_él',\n",
       " 'élè',\n",
       " 'lèv',\n",
       " 'ève',\n",
       " 'ves',\n",
       " '_tem',\n",
       " 'temp',\n",
       " 'emps',\n",
       " 'mps_',\n",
       " '_est',\n",
       " 'est_',\n",
       " '_un_',\n",
       " '_gra',\n",
       " 'gran',\n",
       " 'rand',\n",
       " 'and_',\n",
       " '_maî',\n",
       " 'maît',\n",
       " 'aîtr',\n",
       " 'ître',\n",
       " 'tre_',\n",
       " '_dit',\n",
       " 'dit_',\n",
       " '_on_',\n",
       " '_le_',\n",
       " '_mal',\n",
       " 'malh',\n",
       " 'alhe',\n",
       " 'lheu',\n",
       " 'heur',\n",
       " 'eur_',\n",
       " '_est',\n",
       " 'est_',\n",
       " \"_qu'\",\n",
       " \"qu'i\",\n",
       " \"u'il\",\n",
       " \"'il_\",\n",
       " '_tue',\n",
       " 'tue_',\n",
       " '_ses',\n",
       " 'ses_',\n",
       " '_élè',\n",
       " 'élèv',\n",
       " 'lève',\n",
       " 'èves']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import everygrams\n",
    "\n",
    "def ngram_extractor(sent):\n",
    "    return [''.join(ng) for ng in everygrams(sent.replace(' ', '_ _'), 1, 4)\n",
    "           if ' ' not in ng and '\\n' not in ng and ng != ('_',)]\n",
    "\n",
    "ngram_extractor(s_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guessing the Language\n",
    "The <a href=\"https://www.nltk.org/_modules/nltk/classify/textcat.html#TextCat.guess_language\" target=\"_blank\">guess_languages</a> module calculates language distances. Language distances is calculated by the \"out-of-place\" measure between the text and all languages. It does that by calculating the distance metric for every trigram in the input text to be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fra'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import textcat\n",
    "tc_class = textcat.TextCat()\n",
    "\n",
    "# Find the language with the min distance to the text and return its ISO 639-3 code\n",
    "tc_class.guess_language(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'fra' is the **French** language according to <a href=\"https://iso639-3.sil.org/code_tables/639/data/f\" target=\"_blank\">ISO 639-3</a> \n",
    "So it correctly guessed that the sentence _\"Le temps est un grand maître, dit-on, le malheur est qu'il tue ses élèves\"_ belongs to the French language!\n",
    "\n",
    "To quickly check another language, let's input an english sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eng'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc_class.guess_language('The only thing necessary for the triumph of evil is that good men do nothing.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
